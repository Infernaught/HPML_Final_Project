# HPML_Final_Project

This is a project designed to experiment with GRPO on a T4 GPU in GCP. When running scripts, please make sure you are running them from the directory in which the scripts are stored (i.e., in the training subdirectory, you can run `python grpo_training.py ...`)

All GRPO datasets are stored under their corresponding tasks in the `tasks` subdirectory. The datasets were fetched from Hugging Face and edited using `get_aime_dataset.py` and `get_countdown_dataset.py` in the `tasks/scripts` subdirectory.

The distilled datasets were generated by taking the prompts from the original datasets, passing them to o4-mini, and saving the outputs.

To run the general flow of experiments:

For GRPO training only:
1. Run grpo_training.py using the desired base model and dataset.

For SFT training only:
1. Run sft_training.py using the desired base model and dataset.

For GRPO training after SFT pretraining:
1. Run sft_training.py using the desired base model and dataset.
2. Create an empty HF model repo
3. (Be logged in with a token with HF write permissions) Run upload_adapter.py with your local saved adapter weights
4. Run grpo_training.py using the appropriate base model and your uploaded HF adapter.

For inference:
1. If you are testing a base model, you can simply pass in the HF path to query_model.py
2. If you are testing a trained adapter, you need to upload the adapter to Hugging Face (through upload_adapter.py) then pass the HF path to query_model.py

Our results are saved in `inference/scores`.

Our wandb results are stored in https://wandb.ai/mtt-hpml/projects.